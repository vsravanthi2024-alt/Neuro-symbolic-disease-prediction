{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d961fde-3408-4a15-8db6-9b8c934132a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "# Step 1: Import data from local device\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Use file upload prompt in Colab or replace this with local file path if not using Colab\n",
    "\n",
    "# Load the dataset\n",
    "for file_name in uploaded.keys():\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "\n",
    "# Step 2: Check the data structure\n",
    "print(df.head())  # View the first few rows of the dataset\n",
    "\n",
    "# Step 3: Clean and preprocess the symptoms data\n",
    "# Handle missing values and empty symptoms\n",
    "df['symptoms'] = df['symptoms'].fillna('no symptoms')\n",
    "\n",
    "# Join symptoms into a single string for each row (if symptoms are in lists or multiple entries)\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: ' '.join(str(x).split(',')))\n",
    "\n",
    "# Step 4: Vectorize the symptoms column using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = tfidf_vectorizer.fit_transform(df['symptoms'])\n",
    "\n",
    "# Step 5: Encode the target variable (medical conditions)\n",
    "le_condition = LabelEncoder()\n",
    "y = le_condition.fit_transform(df['medical_condition'].fillna(\"Unknown\"))\n",
    "\n",
    "# Step 6: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Train the Decision Tree Model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Evaluate the Model\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Decision Tree Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le_condition.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le_condition.classes_, yticklabels=le_condition.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision, Recall, F1 Score for Macro and Weighted averages\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "# Display Macro and Weighted Average Metrics\n",
    "print(\"\\nMacro Average Metrics:\")\n",
    "print(f\"Precision: {macro_precision:.2f}, Recall: {macro_recall:.2f}, F1 Score: {macro_f1:.2f}\")\n",
    "\n",
    "print(\"\\nWeighted Average Metrics:\")\n",
    "print(f\"Precision: {weighted_precision:.2f}, Recall: {weighted_recall:.2f}, F1 Score: {weighted_f1:.2f}\")\n",
    "\n",
    "# Step 9: Make Predictions for New Symptoms\n",
    "def predict_condition(new_symptoms):\n",
    "    # Join the input symptoms into a single string\n",
    "    symptoms_str = ' '.join(new_symptoms)\n",
    "\n",
    "    # Vectorize the input symptoms\n",
    "    symptom_vector = tfidf_vectorizer.transform([symptoms_str])\n",
    "\n",
    "    # Predict the condition\n",
    "    predicted_condition_idx = dt.predict(symptom_vector)[0]\n",
    "    predicted_condition = le_condition.inverse_transform([predicted_condition_idx])[0]\n",
    "\n",
    "    # Get the probability of prediction (probability of the predicted class)\n",
    "    predicted_prob = dt.predict_proba(symptom_vector)[0]\n",
    "    predicted_confidence = predicted_prob[predicted_condition_idx]\n",
    "\n",
    "    return predicted_condition, predicted_confidence\n",
    "\n",
    "# Example usage:\n",
    "new_symptoms_input = ['night sweats', 'swollen lymph nodes']\n",
    "predicted_condition, confidence = predict_condition(new_symptoms_input)\n",
    "\n",
    "print(f\"\\nPredicted Disease for input symptoms {new_symptoms_input}: {predicted_condition}\")\n",
    "print(f\"Prediction Confidence: {confidence:.2f}\")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "# Step 1: Import data from local device\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Use file upload prompt in Colab or replace this with local file path if not using Colab\n",
    "\n",
    "# Load the dataset\n",
    "for file_name in uploaded.keys():\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "\n",
    "# Step 2: Check the data structure\n",
    "print(df.head())  # View the first few rows of the dataset\n",
    "\n",
    "# Step 3: Clean and preprocess the symptoms data\n",
    "# Handle missing values and empty symptoms\n",
    "df['symptoms'] = df['symptoms'].fillna('no symptoms')\n",
    "\n",
    "# Join symptoms into a single string for each row (if symptoms are in lists or multiple entries)\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: ' '.join(str(x).split(',')))\n",
    "\n",
    "# Step 4: Vectorize the symptoms column using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = tfidf_vectorizer.fit_transform(df['symptoms'])\n",
    "\n",
    "# Step 5: Encode the target variable (medical conditions)\n",
    "le_condition = LabelEncoder()\n",
    "y = le_condition.fit_transform(df['medical_condition'].fillna(\"Unknown\"))\n",
    "\n",
    "# Step 6: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Train the Random Forest Model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Evaluate the Model\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le_condition.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le_condition.classes_, yticklabels=le_condition.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision, Recall, F1 Score for Macro and Weighted averages\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "# Display Macro and Weighted Average Metrics\n",
    "print(\"\\nMacro Average Metrics:\")\n",
    "print(f\"Precision: {macro_precision:.2f}, Recall: {macro_recall:.2f}, F1 Score: {macro_f1:.2f}\")\n",
    "\n",
    "print(\"\\nWeighted Average Metrics:\")\n",
    "print(f\"Precision: {weighted_precision:.2f}, Recall: {weighted_recall:.2f}, F1 Score: {weighted_f1:.2f}\")\n",
    "\n",
    "# Step 9: Make Predictions for New Symptoms\n",
    "def predict_condition(new_symptoms):\n",
    "    # Join the input symptoms into a single string\n",
    "    symptoms_str = ' '.join(new_symptoms)\n",
    "\n",
    "    # Vectorize the input symptoms\n",
    "    symptom_vector = tfidf_vectorizer.transform([symptoms_str])\n",
    "\n",
    "    # Predict the condition\n",
    "    predicted_condition_idx = rf.predict(symptom_vector)[0]\n",
    "    predicted_condition = le_condition.inverse_transform([predicted_condition_idx])[0]\n",
    "\n",
    "    # Get the probability of prediction (probability of the predicted class)\n",
    "    predicted_prob = rf.predict_proba(symptom_vector)[0]\n",
    "    predicted_confidence = predicted_prob[predicted_condition_idx]\n",
    "\n",
    "    return predicted_condition, predicted_confidence\n",
    "\n",
    "# Example usage:\n",
    "new_symptoms_input = ['healthy', 'no symptoms']\n",
    "predicted_condition, confidence = predict_condition(new_symptoms_input)\n",
    "\n",
    "print(f\"\\nPredicted Disease for input symptoms {new_symptoms_input}: {predicted_condition}\")\n",
    "print(f\"Prediction Confidence: {confidence:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "# Step 1: Import data from local device\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Use file upload prompt in Colab or replace this with local file path if not using Colab\n",
    "\n",
    "# Load the dataset\n",
    "for file_name in uploaded.keys():\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "\n",
    "# Step 2: Check the data structure\n",
    "print(df.head())  # View the first few rows of the dataset\n",
    "\n",
    "# Step 3: Clean and preprocess the symptoms data\n",
    "# Handle missing values and empty symptoms\n",
    "df['symptoms'] = df['symptoms'].fillna('no symptoms')\n",
    "\n",
    "# Join symptoms into a single string for each row (if symptoms are in lists or multiple entries)\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: ' '.join(str(x).split(',')))\n",
    "\n",
    "# Step 4: Vectorize the symptoms column using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = tfidf_vectorizer.fit_transform(df['symptoms'])\n",
    "\n",
    "# Step 5: Encode the target variable (medical conditions)\n",
    "le_condition = LabelEncoder()\n",
    "y = le_condition.fit_transform(df['medical_condition'].fillna(\"Unknown\"))\n",
    "\n",
    "# Step 6: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Train the SVM Model\n",
    "svm = SVC(kernel='linear', random_state=42, probability=True)  # Using linear kernel for simplicity\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Evaluate the Model\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"SVM Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le_condition.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le_condition.classes_, yticklabels=le_condition.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision, Recall, F1 Score for Macro and Weighted averages\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "# Display Macro and Weighted Average Metrics\n",
    "print(\"\\nMacro Average Metrics:\")\n",
    "print(f\"Precision: {macro_precision:.2f}, Recall: {macro_recall:.2f}, F1 Score: {macro_f1:.2f}\")\n",
    "\n",
    "print(\"\\nWeighted Average Metrics:\")\n",
    "print(f\"Precision: {weighted_precision:.2f}, Recall: {weighted_recall:.2f}, F1 Score: {weighted_f1:.2f}\")\n",
    "\n",
    "# Step 9: Make Predictions for New Symptoms\n",
    "def predict_condition(new_symptoms):\n",
    "    # Join the input symptoms into a single string\n",
    "    symptoms_str = ' '.join(new_symptoms)\n",
    "\n",
    "    # Vectorize the input symptoms\n",
    "    symptom_vector = tfidf_vectorizer.transform([symptoms_str])\n",
    "\n",
    "    # Predict the condition\n",
    "    predicted_condition_idx = svm.predict(symptom_vector)[0]\n",
    "    predicted_condition = le_condition.inverse_transform([predicted_condition_idx])[0]\n",
    "\n",
    "    # Get the probability of prediction (probability of the predicted class)\n",
    "    predicted_prob = svm.predict_proba(symptom_vector)[0]\n",
    "    predicted_confidence = predicted_prob[predicted_condition_idx]\n",
    "\n",
    "    return predicted_condition, predicted_confidence\n",
    "\n",
    "# Example usage:\n",
    "new_symptoms_input = ['chronic cough', 'wheezing', 'shortness of breath']\n",
    "predicted_condition, confidence = predict_condition(new_symptoms_input)\n",
    "\n",
    "print(f\"\\nPredicted Disease for input symptoms {new_symptoms_input}: {predicted_condition}\")\n",
    "print(f\"Prediction Confidence: {confidence:.2f}\")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "# Step 1: Import data from local device\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Use file upload prompt in Colab or replace this with local file path if not using Colab\n",
    "\n",
    "# Load the dataset\n",
    "for file_name in uploaded.keys():\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "\n",
    "# Step 2: Check the data structure\n",
    "print(df.head())  # View the first few rows of the dataset\n",
    "\n",
    "# Step 3: Clean and preprocess the symptoms data\n",
    "# Handle missing values and empty symptoms\n",
    "df['symptoms'] = df['symptoms'].fillna('no symptoms')\n",
    "\n",
    "# Join symptoms into a single string for each row (if symptoms are in lists or multiple entries)\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: ' '.join(str(x).split(',')))\n",
    "\n",
    "# Step 4: Vectorize the symptoms column using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = tfidf_vectorizer.fit_transform(df['symptoms'])\n",
    "\n",
    "# Step 5: Encode the target variable (medical conditions)\n",
    "le_condition = LabelEncoder()\n",
    "y = le_condition.fit_transform(df['medical_condition'].fillna(\"Unknown\"))\n",
    "\n",
    "# Step 6: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Train the Gradient Boosting Model\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Evaluate the Model\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Gradient Boosting Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le_condition.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le_condition.classes_, yticklabels=le_condition.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision, Recall, F1 Score for Macro and Weighted averages\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "# Display Macro and Weighted Average Metrics\n",
    "print(\"\\nMacro Average Metrics:\")\n",
    "print(f\"Precision: {macro_precision:.2f}, Recall: {macro_recall:.2f}, F1 Score: {macro_f1:.2f}\")\n",
    "\n",
    "print(\"\\nWeighted Average Metrics:\")\n",
    "print(f\"Precision: {weighted_precision:.2f}, Recall: {weighted_recall:.2f}, F1 Score: {weighted_f1:.2f}\")\n",
    "\n",
    "# Step 9: Make Predictions for New Symptoms\n",
    "def predict_condition(new_symptoms):\n",
    "    # Join the input symptoms into a single string\n",
    "    symptoms_str = ' '.join(new_symptoms)\n",
    "\n",
    "    # Vectorize the input symptoms\n",
    "    symptom_vector = tfidf_vectorizer.transform([symptoms_str])\n",
    "\n",
    "    # Predict the condition\n",
    "    predicted_condition_idx = gb_model.predict(symptom_vector)[0]\n",
    "    predicted_condition = le_condition.inverse_transform([predicted_condition_idx])[0]\n",
    "\n",
    "    # Get the probability of prediction (probability of the predicted class)\n",
    "    predicted_prob = gb_model.predict_proba(symptom_vector)[0]\n",
    "    predicted_confidence = predicted_prob[predicted_condition_idx]\n",
    "\n",
    "    return predicted_condition, predicted_confidence\n",
    "\n",
    "# Example usage:\n",
    "new_symptoms_input = ['bloating', 'nausea', 'vomiting']\n",
    "predicted_condition, confidence = predict_condition(new_symptoms_input)\n",
    "\n",
    "print(f\"\\nPredicted Disease for input symptoms {new_symptoms_input}: {predicted_condition}\")\n",
    "print(f\"Prediction Confidence: {confidence:.2f}\")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "# Step 1: Import data from local device\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Use file upload prompt in Colab or replace this with local file path if not using Colab\n",
    "\n",
    "# Load the dataset\n",
    "for file_name in uploaded.keys():\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "\n",
    "# Step 2: Check the data structure\n",
    "print(df.head())  # View the first few rows of the dataset\n",
    "\n",
    "# Step 3: Clean and preprocess the symptoms data\n",
    "# Handle missing values and empty symptoms\n",
    "df['symptoms'] = df['symptoms'].fillna('no symptoms')\n",
    "\n",
    "# Join symptoms into a single string for each row (if symptoms are in lists or multiple entries)\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: ' '.join(str(x).split(',')))\n",
    "\n",
    "# Step 4: Vectorize the symptoms column using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = tfidf_vectorizer.fit_transform(df['symptoms'])\n",
    "\n",
    "# Step 5: Encode the target variable (medical conditions)\n",
    "le_condition = LabelEncoder()\n",
    "y = le_condition.fit_transform(df['medical_condition'].fillna(\"Unknown\"))\n",
    "\n",
    "# Step 6: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Train the KNN Model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Evaluate the Model\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"KNN Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le_condition.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le_condition.classes_, yticklabels=le_condition.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision, Recall, F1 Score for Macro and Weighted averages\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "# Display Macro and Weighted Average Metrics\n",
    "print(\"\\nMacro Average Metrics:\")\n",
    "print(f\"Precision: {macro_precision:.2f}, Recall: {macro_recall:.2f}, F1 Score: {macro_f1:.2f}\")\n",
    "\n",
    "print(\"\\nWeighted Average Metrics:\")\n",
    "print(f\"Precision: {weighted_precision:.2f}, Recall: {weighted_recall:.2f}, F1 Score: {weighted_f1:.2f}\")\n",
    "\n",
    "# Step 9: Make Predictions for New Symptoms\n",
    "def predict_condition(new_symptoms):\n",
    "    # Join the input symptoms into a single string\n",
    "    symptoms_str = ' '.join(new_symptoms)\n",
    "\n",
    "    # Vectorize the input symptoms\n",
    "    symptom_vector = tfidf_vectorizer.transform([symptoms_str])\n",
    "\n",
    "    # Predict the condition\n",
    "    predicted_condition_idx = knn.predict(symptom_vector)[0]\n",
    "    predicted_condition = le_condition.inverse_transform([predicted_condition_idx])[0]\n",
    "\n",
    "    # Get the probability of prediction (distance to nearest neighbors)\n",
    "    neighbors = knn.kneighbors(symptom_vector, n_neighbors=3)\n",
    "    predicted_confidence = 1 / (1 + neighbors[0].mean())  # Inverse of average distance as confidence score\n",
    "\n",
    "    return predicted_condition, predicted_confidence\n",
    "\n",
    "# Example usage:\n",
    "new_symptoms_input = ['fever', 'chills', 'muscle pain']\n",
    "predicted_condition, confidence = predict_condition(new_symptoms_input)\n",
    "\n",
    "print(f\"\\nPredicted Disease for input symptoms {new_symptoms_input}: {predicted_condition}\")\n",
    "print(f\"Prediction Confidence: {confidence:.2f}\")\n",
    "\n",
    "\"\"\"## **Majority Voting(Hard)**\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd  # Import pandas\n",
    "import io # Import io\n",
    "from google.colab import files # Import files for Colab\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier # Import models again for clarity in this block\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer\n",
    "\n",
    "\n",
    "# --- Data Loading and Preprocessing (Duplicate from previous blocks) ---\n",
    "# This ensures X and y are defined for this block\n",
    "uploaded = files.upload()  # Use file upload prompt in Colab or replace this with local file path if not using Colab\n",
    "\n",
    "# Load the dataset\n",
    "for file_name in uploaded.keys():\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "\n",
    "# Handle missing values and empty symptoms\n",
    "df['symptoms'] = df['symptoms'].fillna('no symptoms')\n",
    "\n",
    "# Join symptoms into a single string for each row (if symptoms are in lists or multiple entries)\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: ' '.join(str(x).split(',')))\n",
    "\n",
    "# Vectorize the symptoms column using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = tfidf_vectorizer.fit_transform(df['symptoms'])\n",
    "\n",
    "# Encode the target variable (medical conditions)\n",
    "le_condition = LabelEncoder()\n",
    "y = le_condition.fit_transform(df['medical_condition'].fillna(\"Unknown\"))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize individual models (Ensure these match the models trained earlier if reusing them)\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm = SVC(probability=True, kernel='rbf', random_state=42)  # required for soft voting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# --- Create Voting Ensemble (Hard Voting) ---\n",
    "estimators = [('dt', dt), ('rf', rf), ('svm', svm), ('gb', gb_model), ('knn', knn)]\n",
    "\n",
    "# Fit the individual models within this block before creating the VotingClassifier\n",
    "# This is crucial if the individual model objects from previous blocks are not available\n",
    "dt.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "voting_hard = VotingClassifier(estimators=estimators, voting='hard')\n",
    "voting_hard.fit(X_train, y_train)\n",
    "y_pred_hard = voting_hard.predict(X_test)\n",
    "\n",
    "# --- Evaluation: Hard Voting ---\n",
    "print(\"\\n--- Hard Voting Ensemble Results ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_hard) * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "# Ensure le_condition is defined and fitted for target_names\n",
    "print(classification_report(y_test, y_pred_hard, target_names=le_condition.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "# Ensure le_condition is defined and fitted for xticklabels and yticklabels\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_hard), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=le_condition.classes_, yticklabels=le_condition.classes_)\n",
    "plt.title(\"Confusion Matrix (Hard Voting)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Additional Metrics (Hard)\n",
    "macro_p, macro_r, macro_f1, _ = precision_recall_fscore_support(y_test, y_pred_hard, average='macro')\n",
    "weighted_p, weighted_r, weighted_f1, _ = precision_recall_fscore_support(y_test, y_pred_hard, average='weighted')\n",
    "print(f\"\\nMacro - Precision: {macro_p:.2f}, Recall: {macro_r:.2f}, F1: {macro_f1:.2f}\")\n",
    "print(f\"Weighted - Precision: {weighted_p:.2f}, Recall: {weighted_r:.2f}, F1: {weighted_f1:.2f}\")\n",
    "\n",
    "\"\"\"## **Soft Voting**\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd  # Import pandas\n",
    "import io # Import io\n",
    "from google.colab import files # Import files for Colab\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "# This ensures X and y are defined for this block\n",
    "uploaded = files.upload()  # Use file upload prompt in Colab or replace this with local file path if not using Colab\n",
    "\n",
    "# Load the dataset\n",
    "for file_name in uploaded.keys():\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "\n",
    "# Handle missing values and empty symptoms\n",
    "df['symptoms'] = df['symptoms'].fillna('no symptoms')\n",
    "\n",
    "# Join symptoms into a single string for each row (if symptoms are in lists or multiple entries)\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: ' '.join(str(x).split(',')))\n",
    "\n",
    "# Vectorize the symptoms column using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = tfidf_vectorizer.fit_transform(df['symptoms'])\n",
    "\n",
    "# Encode the target variable (medical conditions)\n",
    "le_condition = LabelEncoder()\n",
    "y = le_condition.fit_transform(df['medical_condition'].fillna(\"Unknown\"))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize individual models\n",
    "# Ensure models support predict_proba for soft voting\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm = SVC(probability=True, kernel='rbf', random_state=42)  # probability=True is essential for soft voting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=5) # KNN supports predict_proba by default\n",
    "\n",
    "# Create list of estimators\n",
    "estimators = [\n",
    "    ('dt', dt),\n",
    "    ('rf', rf),\n",
    "    ('svm', svm),\n",
    "    ('gb', gb_model),\n",
    "    ('knn', knn)\n",
    "]\n",
    "\n",
    "# Fit the individual models within this block\n",
    "dt.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# --- Create Voting Ensemble (Soft Voting) ---\n",
    "# Use voting='soft' and ensure all base estimators support predict_proba\n",
    "voting_soft = VotingClassifier(estimators=estimators, voting='soft')\n",
    "voting_soft.fit(X_train, y_train)\n",
    "y_pred_soft = voting_soft.predict(X_test)\n",
    "\n",
    "# --- Evaluation: Soft Voting ---\n",
    "print(\"\\n--- Soft Voting Ensemble Results ---\")\n",
    "accuracy_soft = accuracy_score(y_test, y_pred_soft)\n",
    "print(f\"Accuracy: {accuracy_soft * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_soft, target_names=le_condition.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_soft = confusion_matrix(y_test, y_pred_soft)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_soft, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=le_condition.classes_, yticklabels=le_condition.classes_)\n",
    "plt.title(\"Confusion Matrix (Soft Voting)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Additional Metrics (Soft)\n",
    "macro_p_soft, macro_r_soft, macro_f1_soft, _ = precision_recall_fscore_support(y_test, y_pred_soft, average='macro')\n",
    "weighted_p_soft, weighted_r_soft, weighted_f1_soft, _ = precision_recall_fscore_support(y_test, y_pred_soft, average='weighted')\n",
    "print(f\"\\nMacro - Precision: {macro_p_soft:.2f}, Recall: {macro_r_soft:.2f}, F1: {macro_f1_soft:.2f}\")\n",
    "print(f\"Weighted - Precision: {weighted_p_soft:.2f}, Recall: {weighted_r_soft:.2f}, F1: {weighted_f1_soft:.2f}\")\n",
    "\n",
    "\"\"\"## **Gradient Boosting Block**\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "# Step 1: Import data from local device\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Use file upload prompt in Colab or replace this with local file path if not using Colab\n",
    "\n",
    "# Load the dataset\n",
    "for file_name in uploaded.keys():\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "\n",
    "# Step 2: Check the data structure\n",
    "print(df.head())  # View the first few rows of the dataset\n",
    "\n",
    "# Step 3: Clean and preprocess the symptoms data\n",
    "# Handle missing values and empty symptoms\n",
    "df['symptoms'] = df['symptoms'].fillna('no symptoms')\n",
    "\n",
    "# Join symptoms into a single string for each row (if symptoms are in lists or multiple entries)\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: ' '.join(str(x).split(',')))\n",
    "\n",
    "# Step 4: Vectorize the symptoms column using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = tfidf_vectorizer.fit_transform(df['symptoms'])\n",
    "\n",
    "# Step 5: Encode the target variable (medical conditions)\n",
    "le_condition = LabelEncoder()\n",
    "y = le_condition.fit_transform(df['medical_condition'].fillna(\"Unknown\"))\n",
    "\n",
    "# Step 6: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Train the Gradient Boosting Model\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Evaluate the Model\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Gradient Boosting Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le_condition.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le_condition.classes_, yticklabels=le_condition.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate Precision, Recall, F1 Score for Macro and Weighted averages\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "# Display Macro and Weighted Average Metrics\n",
    "print(\"\\nMacro Average Metrics:\")\n",
    "print(f\"Precision: {macro_precision:.2f}, Recall: {macro_recall:.2f}, F1 Score: {macro_f1:.2f}\")\n",
    "\n",
    "print(\"\\nWeighted Average Metrics:\")\n",
    "print(f\"Precision: {weighted_precision:.2f}, Recall: {weighted_recall:.2f}, F1 Score: {weighted_f1:.2f}\")\n",
    "\n",
    "# Step 9: Make Predictions for New Symptoms\n",
    "def predict_condition(new_symptoms):\n",
    "    # Join the input symptoms into a single string\n",
    "    symptoms_str = ' '.join(new_symptoms)\n",
    "\n",
    "    # Vectorize the input symptoms\n",
    "    symptom_vector = tfidf_vectorizer.transform([symptoms_str])\n",
    "\n",
    "    # Predict the condition\n",
    "    predicted_condition_idx = gb_model.predict(symptom_vector)[0]\n",
    "    predicted_condition = le_condition.inverse_transform([predicted_condition_idx])[0]\n",
    "\n",
    "    # Get the probability of prediction (probability of the predicted class)\n",
    "    predicted_prob = gb_model.predict_proba(symptom_vector)[0]\n",
    "    predicted_confidence = predicted_prob[predicted_condition_idx]\n",
    "\n",
    "    return predicted_condition, predicted_confidence\n",
    "\n",
    "# Example usage:\n",
    "new_symptoms_input = ['bloating', 'nausea', 'vomiting']\n",
    "predicted_condition, confidence = predict_condition(new_symptoms_input)\n",
    "\n",
    "print(f\"\\nPredicted Disease for input symptoms {new_symptoms_input}: {predicted_condition}\")\n",
    "print(f\"Prediction Confidence: {confidence:.2f}\")\n",
    "\n",
    "\"\"\"# **Without noise**\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "from google.colab import files\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "uploaded = files.upload()\n",
    "\n",
    "for file_name in uploaded.keys():\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "\n",
    "df['symptoms'] = df['symptoms'].fillna('no symptoms')\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: ' '.join(str(x).split(',')))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = tfidf_vectorizer.fit_transform(df['symptoms'])\n",
    "\n",
    "le_condition = LabelEncoder()\n",
    "y = le_condition.fit_transform(df['medical_condition'].fillna(\"Unknown\"))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Hyperparameter Tuning for Random Forest ---\n",
    "# Define the model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "# cv=5 means 5-fold cross-validation\n",
    "# scoring='accuracy' is the evaluation metric\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# --- Evaluate the Best Model on the Test Set ---\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "print(\"\\nEvaluation of Best Random Forest Model on Test Set:\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le_condition.classes_))\n",
    "\n",
    "# You can also plot the confusion matrix for the best model\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le_condition.classes_, yticklabels=le_condition.classes_)\n",
    "plt.title(\"Confusion Matrix (Tuned Random Forest)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# You would repeat this tuning process for each individual model\n",
    "# and then potentially use the best-tuned models in the VotingClassifier.\n",
    "\n",
    "\"\"\"**Hyperparameter Tuning for Random Forest (Repeat for other models)**\n",
    "\n",
    "**Weighted Soft Voting Ensemble**\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import io\n",
    "from google.colab import files\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "# (Ensure this block is run to define X, y, X_train, X_test, y_train, y_test, le_condition)\n",
    "uploaded = files.upload()\n",
    "\n",
    "for file_name in uploaded.keys():\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "\n",
    "df['symptoms'] = df['symptoms'].fillna('no symptoms')\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: ' '.join(str(x).split(',')))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = tfidf_vectorizer.fit_transform(df['symptoms'])\n",
    "\n",
    "le_condition = LabelEncoder()\n",
    "y = le_condition.fit_transform(df['medical_condition'].fillna(\"Unknown\"))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# --- Initialize and Train Individual Models ---\n",
    "# It's recommended to use the *tuned* best models here if you ran the tuning step\n",
    "dt = DecisionTreeClassifier(random_state=42) # Replace with best_dt_model if tuned\n",
    "rf = RandomForestClassifier(random_state=42) # Replace with best_rf_model if tuned\n",
    "svm = SVC(probability=True, kernel='rbf', random_state=42) # Replace with best_svm_model if tuned\n",
    "gb_model = GradientBoostingClassifier(random_state=42) # Replace with best_gb_model if tuned\n",
    "knn = KNeighborsClassifier(n_neighbors=5) # Replace with best_knn_model if tuned\n",
    "\n",
    "# Fit the individual models\n",
    "dt.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# --- Create and Train Weighted Soft Voting Ensemble ---\n",
    "estimators = [\n",
    "    ('dt', dt),\n",
    "    ('rf', rf),\n",
    "    ('svm', svm),\n",
    "    ('gb', gb_model),\n",
    "    ('knn', knn)\n",
    "]\n",
    "\n",
    "# Assign weights based on perceived performance (you'd typically use cross-validation scores)\n",
    "# Example weights - adjust based on your model's tuning results!\n",
    "weights = [0.1, 0.3, 0.2, 0.3, 0.1] # Sum of weights does not need to be 1\n",
    "\n",
    "voting_soft_weighted = VotingClassifier(estimators=estimators, voting='soft', weights=weights)\n",
    "\n",
    "print(\"\\nStarting Weighted Soft Voting Ensemble training...\")\n",
    "voting_soft_weighted.fit(X_train, y_train)\n",
    "y_pred_soft_weighted = voting_soft_weighted.predict(X_test)\n",
    "\n",
    "# --- Evaluation: Weighted Soft Voting ---\n",
    "print(\"\\n--- Weighted Soft Voting Ensemble Results ---\")\n",
    "accuracy_soft_weighted = accuracy_score(y_test, y_pred_soft_weighted)\n",
    "print(f\"Accuracy: {accuracy_soft_weighted * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_soft_weighted, target_names=le_condition.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_soft_weighted = confusion_matrix(y_test, y_pred_soft_weighted)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_soft_weighted, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=le_condition.classes_, yticklabels=le_condition.classes_)\n",
    "plt.title(\"Confusion Matrix (Weighted Soft Voting)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Additional Metrics (Weighted Soft)\n",
    "macro_p_sw, macro_r_sw, macro_f1_sw, _ = precision_recall_fscore_support(y_test, y_pred_soft_weighted, average='macro')\n",
    "weighted_p_sw, weighted_r_sw, weighted_f1_sw, _ = precision_recall_fscore_support(y_test, y_pred_soft_weighted, average='weighted')\n",
    "print(f\"\\nMacro - Precision: {macro_p_sw:.2f}, Recall: {macro_r_sw:.2f}, F1: {macro_f1_sw:.2f}\")\n",
    "print(f\"Weighted - Precision: {weighted_p_sw:.2f}, Recall: {weighted_r_sw:.2f}, F1: {weighted_f1_sw:.2f}\")\n",
    "\n",
    "\"\"\"## **AdaBoost Classifier**\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "from google.colab import files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier # AdaBoost uses Decision Trees as base estimators\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "# (Ensure this block is run to define X, y, X_train, X_test, y_train, y_test, le_condition)\n",
    "uploaded = files.upload()\n",
    "\n",
    "for file_name in uploaded.keys():\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "\n",
    "df['symptoms'] = df['symptoms'].fillna('no symptoms')\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: ' '.join(str(x).split(',')))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = tfidf_vectorizer.fit_transform(df['symptoms'])\n",
    "\n",
    "le_condition = LabelEncoder()\n",
    "y = le_condition.fit_transform(df['medical_condition'].fillna(\"Unknown\"))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Initialize and Train AdaBoost Classifier ---\n",
    "# Base estimator can be changed, but Decision Tree is common\n",
    "# You might want to tune n_estimators and learning_rate\n",
    "adaboost = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), # Use a decision stump\n",
    "                              n_estimators=100, # Number of boosting rounds\n",
    "                              learning_rate=1.0,\n",
    "                              random_state=42)\n",
    "\n",
    "print(\"\\nStarting AdaBoost training...\")\n",
    "adaboost.fit(X_train, y_train)\n",
    "y_pred_ada = adaboost.predict(X_test)\n",
    "\n",
    "# --- Evaluation: AdaBoost ---\n",
    "print(\"\\n--- AdaBoost Classifier Results ---\")\n",
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "print(f\"Accuracy: {accuracy_ada * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_ada, target_names=le_condition.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_ada = confusion_matrix(y_test, y_pred_ada)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_ada, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=le_condition.classes_, yticklabels=le_condition.classes_)\n",
    "plt.title(\"Confusion Matrix (AdaBoost)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\"## **XGBoost Classifier**\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "from google.colab import files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb # Import XGBoost\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "# (Ensure this block is run to define X, y, X_train, X_test, y_train, y_test, le_condition)\n",
    "uploaded = files.upload()\n",
    "\n",
    "for file_name in uploaded.keys():\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "\n",
    "df['symptoms'] = df['symptoms'].fillna('no symptoms')\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: ' '.join(str(x).split(',')))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = tfidf_vectorizer.fit_transform(df['symptoms'])\n",
    "\n",
    "le_condition = LabelEncoder()\n",
    "y = le_condition.fit_transform(df['medical_condition'].fillna(\"Unknown\"))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Initialize and Train XGBoost Classifier ---\n",
    "# XGBoost parameters can be heavily tuned. This is a basic setup.\n",
    "# objective='multi:softprob' for multi-class classification with probability outputs\n",
    "# use_label_encoder=False is recommended to avoid FutureWarning\n",
    "# eval_metric='mlogloss' is a common metric for multi-class\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softprob',\n",
    "                            num_class=len(le_condition.classes_), # Specify number of classes\n",
    "                            n_estimators=100,\n",
    "                            learning_rate=0.1,\n",
    "                            random_state=42,\n",
    "                            use_label_encoder=False,\n",
    "                            eval_metric='mlogloss')\n",
    "\n",
    "print(\"\\nStarting XGBoost training...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# --- Evaluation: XGBoost ---\n",
    "print(\"\\n--- XGBoost Classifier Results ---\")\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"Accuracy: {accuracy_xgb * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=le_condition.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_xgb, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=le_condition.classes_, yticklabels=le_condition.classes_)\n",
    "plt.title(\"Confusion Matrix (XGBoost)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\"# Stacking Classifier\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "from google.colab import files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression # A common meta-model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "# (Ensure this block is run to define X, y, X_train, X_test, y_train, y_test, le_condition)\n",
    "uploaded = files.upload()\n",
    "\n",
    "for file_name in uploaded.keys():\n",
    "    df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "\n",
    "df['symptoms'] = df['symptoms'].fillna('no symptoms')\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: ' '.join(str(x).split(',')))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = tfidf_vectorizer.fit_transform(df['symptoms'])\n",
    "\n",
    "le_condition = LabelEncoder()\n",
    "y = le_condition.fit_transform(df['medical_condition'].fillna(\"Unknown\"))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Initialize Base Models for Stacking ---\n",
    "# It's highly recommended to use TUNED base models here\n",
    "estimators = [\n",
    "    ('dt', DecisionTreeClassifier(random_state=42)), # Replace with best_dt_model if tuned\n",
    "    ('rf', RandomForestClassifier(random_state=42)), # Replace with best_rf_model if tuned\n",
    "    # SVM can be slow with probability=True in stacking, consider other models\n",
    "    # ('svm', SVC(probability=True, kernel='rbf', random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(random_state=42)), # Replace with best_gb_model if tuned\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)) # Replace with best_knn_model if tuned\n",
    "]\n",
    "\n",
    "# Define the meta-model (final estimator)\n",
    "# Logistic Regression is a simple and often effective choice\n",
    "final_estimator = LogisticRegression(multi_class='auto', solver='liblinear', random_state=42)\n",
    "\n",
    "# --- Create and Train Stacking Classifier ---\n",
    "# cv parameter splits the training data to train base models and the meta-model\n",
    "stacking_model = StackingClassifier(estimators=estimators,\n",
    "                                    final_estimator=final_estimator,\n",
    "                                    cv=5) # Use 5-fold cross-validation\n",
    "\n",
    "print(\"\\nStarting Stacking Ensemble training...\")\n",
    "# StackingClassifier will train the base estimators and the final estimator\n",
    "stacking_model.fit(X_train, y_train)\n",
    "y_pred_stack = stacking_model.predict(X_test)\n",
    "\n",
    "# --- Evaluation: Stacking ---\n",
    "print(\"\\n--- Stacking Classifier Results ---\")\n",
    "accuracy_stack = accuracy_score(y_test, y_pred_stack)\n",
    "print(f\"Accuracy: {accuracy_stack * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_stack, target_names=le_condition.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_stack = confusion_matrix(y_test, y_pred_stack)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_stack, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=le_condition.classes_, yticklabels=le_condition.classes_)\n",
    "plt.title(\"Confusion Matrix (Stacking)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
